{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cd5bb073bb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vaibhav.raj/.local/lib/python3.6/site-packages (2.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.3.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.3)\n",
      "Requirement already satisfied: boto3 in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from transformers) (1.9.251)\n",
      "Requirement already satisfied: tqdm in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: sentencepiece in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.251 in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from boto3->transformers) (1.12.251)\n",
      "Requirement already satisfied: joblib in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied: click in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.251->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/vaibhav.raj/.local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.251->boto3->transformers) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/vaibhav.raj/.local/lib/python3.6/site-packages (20.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained('model_save1')\n",
    "tokenizer = BertTokenizer.from_pretrained('model_save1')\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df1 = pd.read_csv(\"deal_test1.csv\")\n",
    "\n",
    "df1 = df1[df1['SENTENCES'].str.len() <512]\n",
    "\n",
    "df1['LABEL'] = df1['LABEL'].astype(int)\n",
    "\n",
    "df1.head()\n",
    "\n",
    "sentences = df1.SENTENCES.values\n",
    "labels =df1.LABEL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1\n",
      " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LINKS        74\n",
       "SENTENCES    74\n",
       "LABEL        74\n",
       "Queries      12\n",
       "FN           13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type( dict(df1.iloc[0])['LABEL']) )\n",
    "\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum_sentence_length:- 74\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN =256\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "print('Maximum_sentence_length:-',max([len(sen) for sen in input_ids]))\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 30  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 74 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.30837977, -0.18365942],\n",
      "       [ 0.6811949 , -0.6678633 ],\n",
      "       [-1.4250894 ,  0.85718834],\n",
      "       [-0.56047523,  0.2359885 ],\n",
      "       [ 1.534073  , -1.1917535 ],\n",
      "       [ 1.7418637 , -1.3553119 ],\n",
      "       [-1.4937885 ,  0.9425752 ],\n",
      "       [-1.530865  ,  0.8851419 ],\n",
      "       [-1.3353658 ,  0.6926642 ],\n",
      "       [ 1.699431  , -1.1628115 ],\n",
      "       [-0.4885103 ,  0.18428466],\n",
      "       [-0.7694634 ,  0.37012166],\n",
      "       [-0.75417596,  0.36999458],\n",
      "       [-1.2260853 ,  0.77012336],\n",
      "       [ 1.1864306 , -0.9415839 ],\n",
      "       [ 1.4719377 , -0.9795939 ],\n",
      "       [-1.5866519 ,  0.93857545],\n",
      "       [-1.2763113 ,  0.67101055],\n",
      "       [-0.17478517, -0.13741815],\n",
      "       [ 1.3174405 , -1.0245115 ],\n",
      "       [ 0.7727344 , -0.8454611 ],\n",
      "       [-0.67265403,  0.15766264],\n",
      "       [-1.0939124 ,  0.66917425],\n",
      "       [ 1.7244306 , -1.3163557 ],\n",
      "       [ 1.6721957 , -1.2892926 ],\n",
      "       [ 0.46148017, -0.40228555],\n",
      "       [-1.4076521 ,  0.8651201 ],\n",
      "       [ 0.33882448, -0.17439142],\n",
      "       [ 1.6575406 , -1.2526815 ],\n",
      "       [ 1.7879844 , -1.3106508 ]], dtype=float32), array([[ 1.7277248e+00, -1.2056369e+00],\n",
      "       [ 1.5378864e+00, -1.0246236e+00],\n",
      "       [ 1.6799004e+00, -1.2422941e+00],\n",
      "       [-4.5257235e-01,  2.2023091e-01],\n",
      "       [ 1.7909079e+00, -1.3480899e+00],\n",
      "       [-6.8242007e-01,  4.1076267e-01],\n",
      "       [-1.4111106e+00,  9.1684502e-01],\n",
      "       [-9.4494855e-01,  5.3536904e-01],\n",
      "       [-7.7032483e-01,  4.3112612e-01],\n",
      "       [-9.5569825e-01,  4.9009967e-01],\n",
      "       [-1.3784347e+00,  8.5911494e-01],\n",
      "       [ 1.7390456e+00, -1.3023807e+00],\n",
      "       [ 1.6737039e+00, -1.1637743e+00],\n",
      "       [ 1.7455149e+00, -1.3222656e+00],\n",
      "       [-1.1274459e+00,  6.1856401e-01],\n",
      "       [ 1.3429344e+00, -1.0625911e+00],\n",
      "       [-1.1364679e+00,  3.6104482e-01],\n",
      "       [-1.0681918e+00,  5.7001483e-01],\n",
      "       [-1.1252987e+00,  3.7801427e-01],\n",
      "       [-1.4133430e+00,  9.5125860e-01],\n",
      "       [-8.2878125e-01,  4.6206704e-01],\n",
      "       [-1.2651939e+00,  7.5477982e-01],\n",
      "       [ 1.6142160e+00, -1.2421641e+00],\n",
      "       [ 1.3735107e+00, -1.0573491e+00],\n",
      "       [ 1.6518395e+00, -1.2480547e+00],\n",
      "       [ 1.6467795e+00, -1.1976839e+00],\n",
      "       [-3.3008704e-01, -2.3216063e-01],\n",
      "       [-3.0501160e-01,  1.3194121e-03],\n",
      "       [-1.3514490e+00,  7.5564325e-01],\n",
      "       [-9.6128714e-01,  5.9015226e-01]], dtype=float32), array([[-1.4666057 ,  0.7600274 ],\n",
      "       [ 1.6549323 , -1.1770568 ],\n",
      "       [-1.5343715 ,  0.8229979 ],\n",
      "       [-1.5137969 ,  0.91245234],\n",
      "       [ 1.4373081 , -0.9982878 ],\n",
      "       [ 1.2772725 , -0.8974783 ],\n",
      "       [ 0.8972987 , -0.7612586 ],\n",
      "       [ 1.716459  , -1.2525953 ],\n",
      "       [ 1.7291185 , -1.2922096 ],\n",
      "       [ 1.6300302 , -1.2762959 ],\n",
      "       [ 1.2849162 , -1.0011479 ],\n",
      "       [ 1.5471644 , -1.1021312 ],\n",
      "       [-1.383441  ,  0.85253155],\n",
      "       [ 1.4477448 , -1.1153232 ]], dtype=float32)]\n",
      "--------------------------------------------------\n",
      "[array([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 1, 1, 0, 0]), array([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1]), array([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print('-'*50)\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 38 of 74 (51.35%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df1.LABEL.sum(), len(df1.LABEL), (df1.LABEL.sum() / len(df1.LABEL) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fns(preds, labels ,i1):\n",
    "    #'numpy.ndarray' object type\n",
    "    pred_flat = preds.flatten()\n",
    "    #'numpy.ndarray' object type\n",
    "    labels_flat = labels.flatten()\n",
    "  \n",
    "    for j1 in range(len(pred_flat)):\n",
    "        if(pred_flat[j1] == 0 and labels_flat[j1] ==1):\n",
    "            fn_index_list.append(batch_size*i1 + j1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fps(preds, labels, i1):\n",
    "    #'numpy.ndarray' object type\n",
    "    pred_flat = preds.flatten()\n",
    "    #'numpy.ndarray' object type\n",
    "    labels_flat = labels.flatten()\n",
    "  \n",
    "    for j1 in range(len(pred_flat)):\n",
    "        if(pred_flat[j1] == 1 and labels_flat[j1] ==0):\n",
    "            fp_index_list.append(batch_size*i1 + j1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n",
      "[0 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0]\n",
      "--------------------------------------------------\n",
      "[0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0]\n",
      "0.7666666666666667\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1]\n",
      "--------------------------------------------------\n",
      "[0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1]\n",
      "0.9333333333333333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[1 0 1 1 0 0 0 0 0 0 0 0 1 0]\n",
      "--------------------------------------------------\n",
      "[1 0 1 1 0 0 0 0 0 0 0 0 1 0]\n",
      "1.0\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "sum_l,ln =0,0\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "predicted_labels =[]\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    predicted_labels+= list(pred_labels_i)\n",
    "    print(pred_labels_i)\n",
    "    print('-'*50)\n",
    "    print(true_labels[i])\n",
    "\n",
    "    batch_acc = flat_accuracy(pred_labels_i, true_labels[i])\n",
    "    print(batch_acc)\n",
    "    print('~'*50)\n",
    "    sum_l += batch_acc*len(true_labels[i])\n",
    "    ln+= len(true_labels[i])\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.83783783783784\n"
     ]
    }
   ],
   "source": [
    "avg_acc = (sum_l/ln)*100\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5345224838248488, 0.8611111111111112, 1.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 20, 27, 31]\n"
     ]
    }
   ],
   "source": [
    "fn_index_list =[]\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    fns(pred_labels_i, true_labels[i],i)\n",
    "\n",
    "print(fn_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 21, 22, 50]\n"
     ]
    }
   ],
   "source": [
    "fp_index_list =[]\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    fps(pred_labels_i, true_labels[i],i)\n",
    "\n",
    "print(fp_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp 33\n",
      "fp 4\n",
      "fn 5\n",
      "tn 32\n",
      "precision:- 0.8918918918918919\n",
      "recall:- 0.868421052631579\n",
      "f1_score:- 0.88\n",
      "overall_accuracy:- 87.83783783783784\n"
     ]
    }
   ],
   "source": [
    "tp = df1.LABEL.sum() - len(fn_index_list)\n",
    "tn = len(df1.LABEL) - df1.LABEL.sum() - len(fp_index_list)\n",
    "fp =len(fp_index_list)\n",
    "fn = len(fn_index_list)\n",
    "\n",
    "print('tp', tp)\n",
    "print('fp', fp)\n",
    "print('fn', fn)\n",
    "print('tn', tn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "f1_score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('precision:-',precision)\n",
    "print('recall:-',recall)\n",
    "print('f1_score:-',f1_score)\n",
    "print('overall_accuracy:-',avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fb81e110fd0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "doc = nlp('Sun Pharmaceutical recently debuted on the NYSE exchange. It is expected to raise $1 billion with a bond sale after paying completing its $4 billion deal for Ranbaxy Laboratories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Pharmaceutical ORG\n",
      "NYSE ORG\n",
      "$1 billion MONEY\n",
      "$4 billion MONEY\n",
      "Ranbaxy Laboratories ORG\n"
     ]
    }
   ],
   "source": [
    "for x in doc.ents:\n",
    "    print(x.text, x.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sun Pharmaceutical: [Sun Pharmaceutical, It, its]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.has_coref\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sun Pharmaceutical, It, its]\n",
      "its\n",
      "Sun Pharmaceutical\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-6114778b13ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(doc._.coref_clusters[0].mentions)\n",
    "print(doc._.coref_clusters[0].mentions[-1])\n",
    "print(doc._.coref_clusters[0].mentions[-1]._.coref_cluster.main)\n",
    "\n",
    "print('~'*50)\n",
    "print(doc._.coref_clusters[1].mentions)\n",
    "print(doc._.coref_clusters[1].mentions[-1])\n",
    "print(doc._.coref_clusters[1].mentions[-1]._.coref_cluster.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Sun Pharmaceutical: {Sun Pharmaceutical: 0.8751775026321411},\n",
      " the NYSE exchange: {Sun Pharmaceutical: -1.158556342124939,\n",
      "                     the NYSE exchange: 0.4251871109008789,\n",
      "                     NYSE: -1.5458831787109375},\n",
      " NYSE: {Sun Pharmaceutical: -1.5722846984863281, NYSE: 3.334199905395508},\n",
      " It: {Sun Pharmaceutical: 6.316093444824219,\n",
      "      the NYSE exchange: -1.129673719406128,\n",
      "      NYSE: -1.7919954061508179,\n",
      "      It: 0.2531677484512329},\n",
      " a bond sale: {Sun Pharmaceutical: -2.3145387172698975,\n",
      "               the NYSE exchange: -1.9362432956695557,\n",
      "               NYSE: -2.042330265045166,\n",
      "               It: -1.4826641082763672,\n",
      "               a bond sale: 1.732793927192688},\n",
      " paying completing its $4 billion deal for Ranbaxy Laboratories: {Sun Pharmaceutical: -1.9962166547775269,\n",
      "                                                                  the NYSE exchange: -2.0024654865264893,\n",
      "                                                                  NYSE: -2.2155206203460693,\n",
      "                                                                  It: -1.4797275066375732,\n",
      "                                                                  a bond sale: -1.4846147298812866,\n",
      "                                                                  paying completing its $4 billion deal for Ranbaxy Laboratories: 1.6921426057815552},\n",
      " its: {Sun Pharmaceutical: 1.5575802326202393,\n",
      "       the NYSE exchange: -0.6633608341217041,\n",
      "       NYSE: -1.889847755432129,\n",
      "       It: 2.5009868144989014,\n",
      "       a bond sale: -1.4782274961471558,\n",
      "       paying completing its $4 billion deal for Ranbaxy Laboratories: -1.6118319034576416,\n",
      "       its: -1.7832350730895996},\n",
      " its $4 billion deal for Ranbaxy Laboratories: {Sun Pharmaceutical: -2.1848254203796387,\n",
      "                                                the NYSE exchange: -2.0311341285705566,\n",
      "                                                NYSE: -2.1622467041015625,\n",
      "                                                It: -1.4841159582138062,\n",
      "                                                a bond sale: -1.4834599494934082,\n",
      "                                                paying completing its $4 billion deal for Ranbaxy Laboratories: -1.493860125541687,\n",
      "                                                its $4 billion deal for Ranbaxy Laboratories: 1.6075700521469116,\n",
      "                                                its: -1.6919429302215576},\n",
      " Ranbaxy Laboratories: {Sun Pharmaceutical: -2.4047718048095703,\n",
      "                        the NYSE exchange: -1.9432494640350342,\n",
      "                        NYSE: -2.0556387901306152,\n",
      "                        It: -1.7511131763458252,\n",
      "                        a bond sale: -1.5569320917129517,\n",
      "                        paying completing its $4 billion deal for Ranbaxy Laboratories: -1.5124850273132324,\n",
      "                        its: -2.8761534690856934,\n",
      "                        its $4 billion deal for Ranbaxy Laboratories: -1.5169464349746704,\n",
      "                        Ranbaxy Laboratories: 0.9865124225616455}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(doc._.coref_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'looking', 'for', 'a', 'region', 'of', 'central', 'Italy', 'bordering', 'the', 'Adriatic', 'Sea', '.', 'The', 'area', 'is', 'mostly', 'mountainous', 'and', 'includes', 'Mt.', 'Corno', ',', 'the', 'highest', 'peak', 'of', 'the', 'mountain', 'range', '.', 'It', 'also', 'includes', 'many', 'sheep', 'and', 'an', 'Italian', 'entrepreneur', 'has', 'an', 'idea', 'about', 'how', 'to', 'make', 'a', 'little', 'money', 'of', 'them', '.']\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "for token in doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We, a region of central Italy bordering the Adriatic Sea, central Italy bordering the Adriatic Sea, Italy, the Adriatic Sea, The area, Mt. Corno, Mt. Corno, the highest peak of the mountain range, the mountain range, It, many sheep and an Italian entrepreneur, Italian, an Italian entrepreneur, an idea about how to make a little money of them, how to make a little money of them, a little money of them, them]\n"
     ]
    }
   ],
   "source": [
    "print(doc._.coref_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5ddbfae67742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_coref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'main'"
     ]
    }
   ],
   "source": [
    "span = doc[-1:]\n",
    "print(span._.is_coref)\n",
    "print(span._.coref_cluster.main)\n",
    "print(span._.coref_cluster.main._.coref_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[-1]\n",
    "token._.in_coref\n",
    "token._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Michael Cohen: [Michael Cohen, his, his, his, his, he, his]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "doc1 = nlp('The legal pressures facing Michael Cohen are growing in a wide-ranging investigation of his personal business affairs and his work on behalf of his former client, President Trump.  In addition to his work for Mr. Trump, he pursued his own business interests, including ventures in real estate, personal loans and investments in taxi medallions.')\n",
    "doc1._.has_coref\n",
    "doc1._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Michael Cohen, his, his, his, his, he, his]\n",
      "his\n",
      "Michael Cohen\n"
     ]
    }
   ],
   "source": [
    "print(doc1._.coref_clusters[0].mentions)\n",
    "print(doc1._.coref_clusters[0].mentions[-1])\n",
    "print(doc1._.coref_clusters[0].mentions[-1]._.coref_cluster.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Cohen 4\n",
      "his 15\n",
      "his 20\n",
      "his 25\n",
      "his 36\n",
      "he 42\n",
      "his 44\n"
     ]
    }
   ],
   "source": [
    "for x in doc1._.coref_clusters[0].mentions:\n",
    "    print(x, x.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael  REFERS TO Michael Cohen\n",
      "Cohen  REFERS TO Michael Cohen\n",
      "his  REFERS TO Michael Cohen\n",
      "his  REFERS TO Michael Cohen\n",
      "his  REFERS TO Michael Cohen\n",
      "his  REFERS TO Michael Cohen\n",
      "he  REFERS TO Michael Cohen\n",
      "his  REFERS TO Michael Cohen\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    if token._.in_coref:\n",
    "        for cluster in token._.coref_clusters:\n",
    "            print(token.text , \" REFERS TO\", cluster.main.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'own'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlist[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "mongoClient = MongoClient('10.240.0.46', 36018, username='read',\n",
    "                            password='fdfREsse', connectTimeoutMS=100000)\n",
    "\n",
    "db=mongoClient[\"pharma_crawling\"]\n",
    "coll=db['vaibhav_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allergan PERSON\n",
      "$40 billion MONEY\n",
      "Actavis ORG\n",
      "the Justice Department's ORG\n",
      "SEC ORG\n",
      "Thursday DATE\n",
      "June 25 DATE\n",
      "DOJ ORG\n",
      "Antitrust Division ORG\n",
      "Company ORG\n",
      "four CARDINAL\n",
      "Bloomberg PERSON\n",
      "Vermont GPE\n",
      "Bernie Sanders Rapidly PERSON\n",
      "Medicare ORG\n",
      "10% PERCENT\n",
      "a single year DATE\n",
      "between July of 2013 and 2014 DATE\n",
      "Vermont GPE\n",
      "Bernie Sanders PERSON\n",
      "DOJ Antitrust ORG\n",
      "last fall DATE\n",
      "Actavis ORG\n",
      "fourth ORDINAL\n",
      "Impax Laboratories ORG\n",
      "IPXL ORG\n",
      "Lannett PERSON\n",
      "Par Pharmaceutical PERSON\n",
      "DOJ ORG\n",
      "Policy and Regulatory Report ORG\n",
      "DOJ ORG\n",
      "Allergan PERSON\n",
      "DOJ ORG\n",
      "just two days DATE\n",
      "last week DATE\n",
      "Teva PERSON\n",
      "TEVA PERSON\n",
      "$40.5 billion MONEY\n",
      "Allergan PERSON\n",
      "$66 billion MONEY\n",
      "Botox ORG\n",
      "SEC ORG\n",
      "Bloomberg PERSON\n",
      "Allergan PERSON\n",
      "DOJ ORG\n",
      "Allergan PERSON\n",
      "Congress ORG\n",
      "Generics NORP\n",
      "U.S. GPE\n",
      "----------------------------------------------------------------------\n",
      "[Allergan ($AGN): [Allergan ($AGN), its, its, it, The drugmaker, its, it, It, it], Actavis: [Actavis, Actavis, Actavis], the four drugmakers: [the four drugmakers, they], Vermont: [Vermont, Vermont], the industry: [the industry, it], Impax Laboratories ($IPXL), Lannett ($LCI) and Par Pharmaceutical all: [Impax Laboratories ($IPXL), Lannett ($LCI) and Par Pharmaceutical all, their, their, they], the DOJ: [the DOJ, the DOJ, DOJ], trade associations: [trade associations, their], Allergan: [Allergan, it, it, it, it, It, Allergan, Allergan, newly transformed Allergan, Allergan], the Allergan name: [the Allergan name, its]]\n"
     ]
    }
   ],
   "source": [
    "for cur in coll.find().skip(1000).limit(50):\n",
    "    #print(cur['desc'])\n",
    "    #print('~'*60)\n",
    "    doc1 = nlp(cur['desc'])\n",
    "    \n",
    "    for x in doc1.ents:\n",
    "        print(x.text, x.label_)\n",
    "    \n",
    "    print('-'*70)\n",
    "    \n",
    "    doc1._.has_coref\n",
    "    print(doc1._.coref_clusters)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$2.5 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"It's discussing a $2.5 billion buyoutof ZS Pharma\")\n",
    "for x1 in doc2.ents:\n",
    "    print(x1.text,x1.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "st1 = CoreNLPParser(url=\"http://localhost:9000\", tagtype='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Horizon', 'O'), ('hikes', 'O'), ('Depomed', 'ORGANIZATION'), ('offer', 'O'), ('to', 'O'), ('$', 'MONEY'), ('2B', 'MONEY'), ('in', 'O'), ('bid', 'O'), ('for', 'O'), ('`', 'O'), ('friendly', 'O'), (\"'\", 'O'), ('deal', 'O'), ('talks', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "op = st1.tag_sents([[\"Horizon hikes Depomed offer to $2B in bid for 'friendly' deal talks\"]])\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] 74\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels,len(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "output =[]\n",
    "for i in range(len(sentences) ):\n",
    "    d ={}\n",
    "    org_set,money_set =set(),set()\n",
    "    if(predicted_labels[i] == 1):\n",
    "        tup_list = st1.tag_sents([[sentences[i]]])\n",
    "#         print(tup_list)\n",
    "        val,temp ='',''\n",
    "        for j in tup_list[0]:\n",
    "            if(j[1] == 'O' and temp):\n",
    "                if(temp == 'MONEY' and val and not re.search('[$] [A-Za-z]+',val) ):\n",
    "                    money_set.add(val.lower())\n",
    "                elif((temp == 'ORGANIZATION' or temp == 'PERSON') and val and not re.search('[$] [A-Za-z]+',val)):\n",
    "                    org_set.add(val)\n",
    "                \n",
    "                val =''\n",
    "            \n",
    "            if(j[1]!= 'O'):\n",
    "                val+= j[0]+ ' '\n",
    "                temp = j[1]\n",
    "                \n",
    "        if(temp):\n",
    "            if(temp == 'MONEY' and val and not re.search('[$] [A-Za-z]+',val) ):\n",
    "                money_set.add(val.lower())\n",
    "            elif((temp == 'ORGANIZATION' or temp == 'PERSON') and val and not re.search('[$] [A-Za-z]+',val)):\n",
    "                org_set.add(val)\n",
    "#         print(sentences[i])\n",
    "#         print(org_list)\n",
    "#         print(money_list)\n",
    "        \n",
    "    d['sentence'] = sentences[i]\n",
    "    d['status'] = predicted_labels[i]\n",
    "    d['organizations/persons'] = list(org_set)\n",
    "    d['money'] = list(money_set)\n",
    "    output.append(d)\n",
    "\n",
    "#print(output)    \n",
    "df_output = pd.DataFrame(output)\n",
    "#print(df_output)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>status</th>\n",
       "      <th>organizations/persons</th>\n",
       "      <th>money</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Allergan ($AGN) may be shedding its background...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Allergan received the DOJ inquiry just two day...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>It recently took on the Allergan name after co...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Allergan ]</td>\n",
       "      <td>[$ 66 billion ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Horizon has offered $2 billion for Depomed, bu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Depomed ]</td>\n",
       "      <td>[$ 2 billion ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The Hyperion acquisition was completed on May ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Adjusted operating cash flow in the second qua...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Merck sold Bayer its consumer business for $14...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Bayer , Merck ]</td>\n",
       "      <td>[$ 14 billion ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Sanofi deepens its Evotec ties with a $330M di...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Sanofi ]</td>\n",
       "      <td>[$ 330m ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Teva Reinforces Leadership Position in Respira...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Teva , Acquisition of Gecko Health Innovations ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>As Leerink Partners' Jason Gerberry wrote in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  status  \\\n",
       "0  Allergan ($AGN) may be shedding its background...       0   \n",
       "1  Allergan received the DOJ inquiry just two day...       0   \n",
       "2  It recently took on the Allergan name after co...       1   \n",
       "3  Horizon has offered $2 billion for Depomed, bu...       1   \n",
       "4  The Hyperion acquisition was completed on May ...       0   \n",
       "5  Adjusted operating cash flow in the second qua...       0   \n",
       "6  Merck sold Bayer its consumer business for $14...       1   \n",
       "7  Sanofi deepens its Evotec ties with a $330M di...       1   \n",
       "8  Teva Reinforces Leadership Position in Respira...       1   \n",
       "9  As Leerink Partners' Jason Gerberry wrote in a...       0   \n",
       "\n",
       "                               organizations/persons            money  \n",
       "0                                                 []               []  \n",
       "1                                                 []               []  \n",
       "2                                        [Allergan ]  [$ 66 billion ]  \n",
       "3                                         [Depomed ]   [$ 2 billion ]  \n",
       "4                                                 []               []  \n",
       "5                                                 []               []  \n",
       "6                                   [Bayer , Merck ]  [$ 14 billion ]  \n",
       "7                                          [Sanofi ]        [$ 330m ]  \n",
       "8  [Teva , Acquisition of Gecko Health Innovations ]               []  \n",
       "9                                                 []               []  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv('deal_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
